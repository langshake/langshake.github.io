# A Dual-Layer Framework for AI-Optimized Web Crawling and Structured Content Delivery

**Whitepaper: Langshake & Extended Sitemap Protocol**  
**Version: 1.0 (April 2025)**  
**Authors: Langshake team**

## Abstract

The rise of Large Language Models (LLMs) and AI agents is reshaping how content is consumed online. Traditional HTML-based crawling methods are inefficient, expensive, and prone to hallucination or misinterpretation. This whitepaper proposes a hybrid standard: combining the Langshake micro-standard with an Extended Sitemap Protocol. Together, these frameworks provide both global and per-page LLM-ready structured content in JSON format. The proposal includes a verification system using hash validation or Merkle roots to ensure data integrity and defend against schema manipulation. This dual approach offers developers, AI systems, and search engines a path toward a scalable, efficient, and trustworthy machine-readable web.

## 1. Purpose & Motivation

Web crawling today involves parsing unstructured HTML, which is costly and imprecise. While Schema.org and JSON-LD have helped, they often bloat pages and are easily manipulated. AI and LLMs require structured, reliable content at scale — but the current ecosystem doesn't deliver it efficiently.

The combination of Langshake and the Extended Sitemap Protocol aims to:

- Reduce server and compute overhead for AI agents and crawlers
- Give developers full control over what content is exposed to machines
- Ensure integrity via hashes and verifiable structures
- Support both whole-site and per-page metadata

## 2. Core Components

### 2.1 Langshake .well-known/llm.json

Langshake is a global summary format that provides LLM-friendly site metadata, FAQs, links to modular JSON content files, and optional LLM-specific context. These files are typically generated by the Langshake CLI, which extracts Schema.org-compliant data from HTML and outputs it as standalone files. Example:

```json
{
  "version": "1.0",
  "site": {
    "name": "Example Corp",
    "description": "Tools for developers",
    "language": "en"
  },
  "modules": [
    "/langshake/article.json",
    "/langshake/products.json",
    "/langshake/docs.json"
  ],
  "llm_context": {
    "summary": "We are a privacy-first AI tools company.",
    "principles": ["Transparency", "Open Source", "Non-tracking"],
    "usage_notes": "We prioritize ethical AI use cases and do not support military applications."
  },
  "verification": {
    "strategy": "merkle",
    "merkleRoot": "abc123...",
    "lastVerified": "2025-04-10"
  }
}
```

⚠️ While the `llm_context` field allows developers to share nuanced or mission-driven content with LLMs, it is not validated by external schema verification. It is recommended that AI platforms and agents treat this field with caution and avoid using it for factual grounding unless verified through external trust or citation mechanisms.

### 2.2 LangShake Extended Sitemap Protocol

Extends sitemap.xml with the `<langshake:schema-url>` and `<langshake:checksum>` tags, using the langshake namespace to provide per-page Schema.org-compliant JSON references and integrity verification. Example:

```xml
<url>
  <loc>https://example.com/article</loc>
  <lastmod>2025-04-16T17:58:00Z</lastmod>
  <changefreq>weekly</changefreq>
  <priority>0.8</priority>
  <langshake:schema-url>https://example.com/langshake/article.json</langshake:schema-url>
  <langshake:checksum>8f7a9b3cf5a...a8b9c07e8f</langshake:checksum>
</url>
```

And in article.json:

```json
{
  "@context": "http://schema.org",
  "@type": "Article",
  "headline": "LangShake: Revolutionizing LLM Training Data",
  "description": "A comprehensive guide to implementing the LangShake protocol for efficient AI data collection",
  "articleBody": "The rise of Large Language Models has created a need for more efficient data collection methods. LangShake addresses this by...",
  "datePublished": "2025-04-16T17:58:00Z",
  "dateModified": "2025-04-16T17:58:00Z",
  "image": "https://example.com/images/langshake-header.jpg",
  "author": {
    "@type": "Person",
    "name": "Jane Smith",
    "url": "https://example.com/team/jane-smith"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Example Corp",
    "logo": {
      "@type": "ImageObject",
      "url": "https://example.com/logo.png"
    }
  },
  "keywords": ["LLM", "AI", "data collection", "web standards"],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://example.com/article"
  },
  "checksum": "8f7a9b3cf5a...a8b9c07e8f"
}
```

This JSON is generated from the embedded schema in the source HTML. During validation, the hash is calculated by extracting the HTML schema excluding the checksum field, to avoid circular references.

## 3. Verification Strategy 

To prevent abuse (fake prices, fake reviews), both systems support validation:

- **Per-JSON Hashing**: Each JSON file includes `checksum` generated from the embedded schema block in the HTML. During validation, this field must be excluded from the hash calculation.

- **Merkle Root Aggregation**: The CLI aggregates all per-page hashes into a Merkle tree, and includes the root in .llm.json. This enables fast multi-file verification.

- **Random Sampling**: Crawlers validate 5–10% of pages by extracting the schema from the actual HTML and comparing the hash.

- **Validation Rules**: If the hashes don't match, crawlers may de-rank the data, issue warnings, or block further trust in that site's structured content.

- **Caution with Context Fields**: Fields like `llm_context` are unverified and should not be used for factual reasoning or truth-grounding by default.

## 4. Developer Workflow

1. Author schema data in source code or CMS.

2. Run `langshake build`:
   - Extracts and caches schema data
   - Generates JSON files in the `/langshake/` directory
   - Computes `checksum` per file (excluding that field)
   - Builds Merkle tree
   - Outputs `.llm.json` with root and optional context

3. Validate with `langshake validate`:
   - Re-extracts schema from source HTML
   - Recalculates hash and compares with published `checksum`
   - Skips cache during strict validation

4. Optional: Add discovery line in robots.txt:
   - `llm-json: /.well-known/llm.json`

## 5. Benefits

- **Efficiency**: Minimizes HTML parsing with structured JSON metadata, reducing computational overhead and speeding up data collection processes.

- **Integrity**: Checksum verification ensures trustworthy data by validating content hasn't been modified, creating a more reliable training dataset.

- **Compatibility**: Builds on established Sitemap.xml and Schema.org standards, making adoption straightforward for websites already using these technologies.

- **Developer-Friendly**: CLI tool abstracts complexity; caching speeds up local builds.

- **Context-Ready**: Developers can add nuance for LLMs in a dedicated field.

- **Future-Proof**: Modular, extendable spec allows for Merkle, LLM summaries, and more.

## 6. Next Steps

- Finalize Langshake Spec + Merkle SDK
- Submit Sitemap extension draft to W3C
- Open-source langshake-cli, langshake-validate
- Partner with AI platforms and SEO tools

## Conclusion

By merging Langshake's developer-first JSON interface with the Extended Sitemap Protocol's page-level precision and hash validation, this proposal delivers a unified standard for AI-native web content. It empowers developers, protects against abuse, and enables the next generation of trusted, structured, machine-readable websites.

## Contact & Contribution

To get involved, visit: [github.com/langshake](https://github.com/langshake) or contact the authors directly. All specs and tools are MIT-licensed and open for contribution. 